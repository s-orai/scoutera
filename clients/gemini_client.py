from google.genai import types
from google import genai
import json
from contextlib import contextmanager
import time
import concurrent.futures

from config import get_gemini_config
# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models.scout_models import (
    AiResult,
    ResultsContainer,
    CreatePromptModel,
    ScoutMaterialModel
)
from models.jd_models import (
    OfferingContentModel,
    BussinessDescriptionModel
)
from models.screening_models import (
    ScreeningResult,
)
# ãƒ¢ãƒ‡ãƒ«ã‚’å¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ã™ã‚‹
__all__ = [
    # Functions
    "request_for_create_prompt",
    "request_with_files_by_parallel",
    "request_with_files_for_scout_material",
    "request_business_description",
    "request_with_files_for_jd",
    # Models (backward compatibility)
    "AiResult",
    "ResultsContainer",
    "CreatePromptModel",
    "ScoutMaterialModel",
    "OfferingContentModel",
    "BussinessDescriptionModel",
    "ScreeningResult",
]

_gemini_config = get_gemini_config()
api_key = _gemini_config["api_key"]
model = _gemini_config["model"]
max_retries = _gemini_config["max_retries"]
backoff_seconds = _gemini_config["backoff_seconds"]

client = genai.Client(api_key=api_key)
n_trials = 3


def _execute_with_retry(api_call):
  """
  ãƒªãƒˆãƒ©ã‚¤ãƒ»ãƒãƒƒã‚¯ã‚ªãƒ•ãƒ»ä¾‹å¤–å‡¦ç†ã‚’å…±é€šåŒ–ã—ãŸAPIå®Ÿè¡Œãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚
  api_call ã¯å¼•æ•°ãªã—ã§å‘¼ã°ã‚Œã€1å›åˆ†ã® generate_content ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ callable ã‚’æ¸¡ã™ã€‚
  """
  for attempt in range(1, max_retries + 1):
    try:
      response = api_call()
      return response
    except json.JSONDecodeError as e:
      print(f"JSON Parse Error: {str(e)}")
      print("Invalid JSON content:", response.text if 'response' in locals() else '')
      raise
    except Exception as e:
      if attempt == max_retries:
        print(f"âŒ Gemini APIã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (attempt {attempt}/{max_retries}): {e}")
        raise
      print(f"âš ï¸ Gemini APIã‚¨ãƒ©ãƒ¼ (attempt {attempt}/{max_retries}): {e} - {backoff_seconds}ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
      time.sleep(backoff_seconds)


def request_for_create_prompt(prompt, files, job_file, temperature=0.2):
  print("--- å‡¦ç†é–‹å§‹ ---")
  start_time = time.time()

  ## response_schemaã®ä½œæˆ
  # Pydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰JSONã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
  response_schema = CreatePromptModel.model_json_schema()
  config = types.GenerateContentConfig(
    system_instruction=prompt,
    # JSONå½¢å¼ã§ã®å‡ºåŠ›ã‚’å¼·åˆ¶
    response_mime_type="application/json",
    response_schema = response_schema,
    temperature = temperature
  )

  with file_uploader(files, job_file) as (uploaded_files, uploaded_job_files):
    response = request(uploaded_files, uploaded_job_files, config)
    result = CreatePromptModel.model_validate_json(response.text)
    print(result)

  end_time = time.time()
  print("--- ä¸¦åˆ—å‡¦ç†çµ‚äº† ---")
  print(f"åˆè¨ˆå®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
  print(f"result: {result}")
  return result

def request(pdfs, job_pdfs, config):
  def _call():
    if isinstance(pdfs, list):
      contents = pdfs + list(job_pdfs)
    else:
      contents = [pdfs] + list(job_pdfs)
    return client.models.generate_content(
        model=model,
        contents=contents,
        config=config
    )
  return _execute_with_retry(_call)

def request_with_files_by_parallel(prompt, files, job_file, response_model, temperature=0.2, is_screening=False):
  """
  PDFãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦Gemini APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹
  
  Args:
      prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
      files: å€™è£œè€…PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
      job_file: æ±‚äººç¥¨PDFãƒ•ã‚¡ã‚¤ãƒ«
      response_model: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®Pydanticãƒ¢ãƒ‡ãƒ«
      temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
      is_screening: Trueã®å ´åˆã¯screeningãƒ¢ãƒ¼ãƒ‰ï¼ˆå…¨ä½“ã‚’3å›å®Ÿè¡Œï¼‰ã€Falseã®å ´åˆã¯scoutãƒ¢ãƒ¼ãƒ‰ï¼ˆå„PDFã‚’3å›ãšã¤å®Ÿè¡Œï¼‰
  
  Returns:
      å‡¦ç†çµæœã®ãƒªã‚¹ãƒˆ
  """
  print("--- ä¸¦åˆ—å‡¦ç†é–‹å§‹ ---")
  start_time = time.time()

  ## response_schemaã®ä½œæˆ
  # Pydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰JSONã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
  response_schema = response_model.model_json_schema()
  config = types.GenerateContentConfig(
    system_instruction=prompt,
    # JSONå½¢å¼ã§ã®å‡ºåŠ›ã‚’å¼·åˆ¶
    response_mime_type="application/json",
    response_schema = response_schema,
    temperature = temperature
  )

  with file_uploader(files, job_file) as (uploaded_files, uploaded_job_files):
    if is_screening:
      results = _parallel_process_for_screening(uploaded_files, uploaded_job_files, config, response_model)
    else:
      results = _parallel_process_for_scout(uploaded_files, uploaded_job_files, config, response_model)

  end_time = time.time()
  print("--- ä¸¦åˆ—å‡¦ç†çµ‚äº† ---")
  print(f"åˆè¨ˆå®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
  print(f"results: {results}")
  return results

def _parallel_process_for_scout(pdf_list, job_pdf_list, config, response_model):
    """
    Scoutç”¨: PDFãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€å„PDFã«å¯¾ã—ã¦3å›ãšã¤APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€
    çµæœã‚’ä¸€ã¤ã®ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã¦è¿”ã—ã¾ã™ã€‚
    """
    all_requests = [
        (pdf_path, attempt_num)
        for pdf_path in pdf_list
        for attempt_num in range(1, 4)
    ]

    MAX_WORKERS = 10
    results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_request = {
            executor.submit(request, pdf_path, job_pdf_list, config): (pdf_path, attempt_num)
            for pdf_path, attempt_num in all_requests
        }

        for future in concurrent.futures.as_completed(future_to_request):
            pdf_path, attempt_num = future_to_request[future]
            try:
                response = future.result()
                result = response_model.model_validate_json(response.text)
                print(result)
                results.append(result)
            except Exception as exc:
                print(f"PDF: {pdf_path}, è©¦è¡Œ: {attempt_num} ã®å®Ÿè¡Œä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}")

    return results

def _parallel_process_for_screening(pdf_list, job_pdf, config, response_model):
    """
    Screeningç”¨: PDFãƒªã‚¹ãƒˆå…¨ä½“ã«å¯¾ã—ã¦3å›APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€
    çµæœã‚’ä¸€ã¤ã®ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã¦è¿”ã—ã¾ã™ã€‚
    
    Args:
        pdf_list: å€™è£œè€…PDFã®ãƒªã‚¹ãƒˆ
        job_pdf: æ±‚äººç¥¨PDFã®ãƒªã‚¹ãƒˆ
        config: APIè¨­å®š
        response_model: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
    
    Returns:
        List[response_model]: 3å›ã®å®Ÿè¡Œçµæœã®ãƒªã‚¹ãƒˆ
    """
    MAX_WORKERS = 10
    results = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # PDFãƒªã‚¹ãƒˆå…¨ä½“ã‚’3å›å®Ÿè¡Œ
        futures = [
            executor.submit(request, pdf_list, job_pdf, config)
            for _ in range(3)
        ]

        for i, future in enumerate(concurrent.futures.as_completed(futures), 1):
            try:
                response = future.result()
                result = response_model.model_validate_json(response.text)
                print(f"âœ… å®Ÿè¡Œ {i}/3 å®Œäº†")
                print(result)
                results.append(result)
            except Exception as exc:
                print(f"âŒ å®Ÿè¡Œ {i}/3 ã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}")

    return results

def request_with_files_for_scout_material(prompt, files, temperature=0.2):
  print("--- å‡¦ç†é–‹å§‹ ---")
  start_time = time.time()

  ## response_schemaã®ä½œæˆ
  # Pydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰JSONã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
  response_schema = ScoutMaterialModel.model_json_schema()
  config = types.GenerateContentConfig(
    system_instruction=prompt,
    # JSONå½¢å¼ã§ã®å‡ºåŠ›ã‚’å¼·åˆ¶
    response_mime_type="application/json",
    response_schema = response_schema,
    temperature = temperature
  )

  with file_uploader(files, []) as (uploaded_files, _):
    response = _request(uploaded_files, config)

  end_time = time.time()
  print(f"åˆè¨ˆå®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ï¼ˆlogicå´ã§tuple/listã¨å‹˜é•ã„ã—ãªã„ã‚ˆã†ã«ï¼‰
  result = ScoutMaterialModel.model_validate_json(response.text)
  print(f"result: {result}")
  return result

# ----------------------------
# ã“ã“ã‹ã‚‰ã¯jdç”¨
# ï¼Šå¾Œã§çµ±åˆã™ã‚‹ã“ã¨ï¼ï¼
# ----------------------------

def request_business_description(prompt, temperature):
  print("--- å‡¦ç†é–‹å§‹ ---")
  start_time = time.time()

  ## response_schemaã®ä½œæˆ
  # Pydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰JSONã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
  response_schema = BussinessDescriptionModel.model_json_schema()
  config = types.GenerateContentConfig(
    # JSONå½¢å¼ã§ã®å‡ºåŠ›ã‚’å¼·åˆ¶
    response_mime_type="application/json",
    response_schema = response_schema,
    temperature = temperature
  )

  response = _request_only_config(config, prompt)
  result = BussinessDescriptionModel.model_validate_json(response.text)

  end_time = time.time()
  print(f"åˆè¨ˆå®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
  return result

def request_with_files_for_jd(prompt, files, temperature):
  print("--- å‡¦ç†é–‹å§‹ ---")
  start_time = time.time()

  ## response_schemaã®ä½œæˆ
  # Pydanticãƒ¢ãƒ‡ãƒ«ã‹ã‚‰JSONã‚¹ã‚­ãƒ¼ãƒã‚’å–å¾—
  response_schema = OfferingContentModel.model_json_schema()
  config = types.GenerateContentConfig(
    system_instruction=prompt,
    # JSONå½¢å¼ã§ã®å‡ºåŠ›ã‚’å¼·åˆ¶
    response_mime_type="application/json",
    response_schema = response_schema,
    temperature = temperature
  )

  with file_uploader(files, []) as (uploaded_files, _):
    response = _request(uploaded_files, config)

  end_time = time.time()
  print(f"åˆè¨ˆå®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ï¼ˆlogicå´ã§tuple/listã¨å‹˜é•ã„ã—ãªã„ã‚ˆã†ã«ï¼‰
  result = OfferingContentModel.model_validate_json(response.text)
  print(f"result: {result}")
  return result

def _request_only_config(config, prompt):
  return _execute_with_retry(
      lambda: client.models.generate_content(
          model=model,
          config=config,
          contents=[prompt]
      )
  )

def _request(pdfs, config):
  return _execute_with_retry(
      lambda: client.models.generate_content(
          model=model,
          contents=pdfs,
          config=config
      )
  )
# ----------------------------
# å…±é€šé–¢æ•°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
# ----------------------------
@contextmanager
def file_uploader(files, job_file=None, is_round: bool = False):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Gemini APIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€'with' ãƒ–ãƒ­ãƒƒã‚¯ã« (uploaded_files, uploaded_job_files) ã‚’æä¾›ã—ã¾ã™ã€‚
    ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†æ™‚ã«ã€æˆåŠŸãƒ»å¤±æ•—ã«é–¢ã‚ã‚‰ãšå¿…ãšã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

    Args:
        files: [(ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹, ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å), ...] ã®ãƒªã‚¹ãƒˆï¼ˆå€™è£œè€…ç”¨ãªã©ï¼‰
        job_file: æ±‚äººç¥¨ç”¨ã® [(ãƒ‘ã‚¹, åå‰), ...]ã€‚çœç•¥æ™‚ã¯ç©ºãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†ï¼ˆå€™è£œè€…ã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰
        is_round: True ã®ã¨ãã¯ (uploaded_files + uploaded_job_files) ã‚’1ã¤ã®ãƒªã‚¹ãƒˆã§ yield
    """
    job_file = job_file if job_file is not None else []
    uploaded_files = []
    uploaded_job_files = []
    try:
      for path, original_name in files:
        uploaded = client.files.upload(file=path)
        uploaded_files.append(uploaded)
        print(f"  ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {original_name}")

      for path, original_name in job_file:
        uploaded = client.files.upload(file=path)
        uploaded_job_files.append(uploaded)
        print(f"  æ±‚äººç¥¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {original_name}")

      if not is_round:
        yield (uploaded_files, uploaded_job_files)
      else:
        uploaded_files.extend(uploaded_job_files)
        yield uploaded_files

    except Exception as e:
        print(f"Error during file processing: {e}")
        raise

    finally:
      for uploaded in uploaded_files + uploaded_job_files:
        try:
          print(f"ğŸ—‘ï¸ Gemini Filesã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ« ({uploaded.name}) ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
          client.files.delete(name=uploaded.name)
          print("âœ… å‰Šé™¤å®Œäº†ã€‚")
        except Exception as delete_error:
          print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {delete_error}")